{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0384f-4be9-42b8-86a1-c57374aa5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import kornia\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import torch.distributed as dist\n",
    "from elasticdino.model.elasticdino import ElasticDino\n",
    "\n",
    "\n",
    "%reload_ext tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92537a-424b-4a33-ad77-8a3cf1653d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERSIM_PATHS = []\n",
    "N_HYPERSIM_IMAGES = 6\n",
    "HYPERSIM_BASE_PATH = \"path/to/hypersim\"\n",
    "\n",
    "for path in os.listdir(HYPERSIM_BASE_PATH):\n",
    "    for subpath in os.listdir(os.path.join(HYPERSIM_BASE_PATH, path, \"images\")):\n",
    "        frames = os.listdir(os.path.join(HYPERSIM_BASE_PATH, path, \"images\", subpath))\n",
    "        frames = [x for x in frames if \"color\" in x]\n",
    "        for f in frames:\n",
    "            HYPERSIM_PATHS.append(os.path.join(HYPERSIM_BASE_PATH, path, \"images\", subpath, f))\n",
    "\n",
    "TRAIN_PROPORTION = 0.8\n",
    "\n",
    "train_size = int(len(HYPERSIM_PATHS) * TRAIN_PROPORTION)\n",
    "HYPERSIM_TRAIN_PATHS = HYPERSIM_PATHS[:train_size]\n",
    "HYPERSIM_TEST_PATHS = HYPERSIM_PATHS[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da92ce55-b6da-4081-97a5-364be3060eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 128\n",
    "\n",
    "def process_image(img):\n",
    "    l = min(img.height, img.width)\n",
    "    return img.convert(\"RGB\").crop((0, 0, l, l)).resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "    \n",
    "def hypersim_sample(p):\n",
    "    folder = os.path.dirname(p)\n",
    "    f = p.split(\"/\")[-1].split(\".\")[1]\n",
    "    \n",
    "    img = process_image(Image.open(os.path.join(folder, f\"frame.{f}.color.jpg\")))\n",
    "    albedo = process_image(Image.open(os.path.join(folder, f\"frame.{f}.diffuse_reflectance.jpg\")))\n",
    "    shading = process_image(Image.open(os.path.join(folder, f\"frame.{f}.diffuse_illumination.jpg\")))\n",
    "    normal = process_image(Image.open(os.path.join(folder.replace(\"final\", \"geometry\"), f\"frame.{f}.normal_bump_cam.png\")))\n",
    "    img =  torchvision.transforms.functional.pil_to_tensor(img)/255.0\n",
    "    albedo = torchvision.transforms.functional.pil_to_tensor(albedo)/255.0\n",
    "    shading = torchvision.transforms.functional.pil_to_tensor(shading)/255.0\n",
    "    normal = torchvision.transforms.functional.pil_to_tensor(normal)/255.0\n",
    "    # normal = (2.0 * torchvision.transforms.functional.pil_to_tensor(normal)/255.0) - 1\n",
    "    return img, albedo, shading, normal\n",
    "\n",
    "class HypersimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        return hypersim_sample(path)\n",
    "\n",
    "\n",
    "\n",
    "hypersim_train_ds = HypersimDataset(HYPERSIM_TRAIN_PATHS)\n",
    "hypersim_test_ds = HypersimDataset(HYPERSIM_TEST_PATHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b2d45d-a350-40a2-9bf4-dd4bf50dd57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, batch_size):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd1906-4f6e-4c63-87c6-0db0883ad330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BITS_AND_BYTES = False\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed, DistributedDataParallelKwargs\n",
    "from elasticdino.model.layers import ResidualBlock, Activation, ProjectionLayer\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, elasticdino):\n",
    "        super().__init__()\n",
    "        elasticdino.requires_grad_ = False\n",
    "        self.elasticdino = elasticdino.eval()\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            ProjectionLayer(3, 256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "        )\n",
    "        self.neck = nn.Sequential(\n",
    "            ProjectionLayer(1024 + 256, 256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "        )\n",
    "        def make_head():\n",
    "            return nn.Sequential(\n",
    "                ResidualBlock(256),\n",
    "                ResidualBlock(256),\n",
    "                nn.Conv2d(256, 128, 1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 64, 1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 3, 1),\n",
    "            )\n",
    "\n",
    "        self.albedo_head = make_head()\n",
    "        self.shading_head = make_head()\n",
    "        self.normal_head = make_head()\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            f = self.elasticdino(x)\n",
    "        x = self.image_encoder(x)\n",
    "        f = self.neck(torch.cat([x, f], dim=1))\n",
    "        return self.albedo_head(f), self.shading_head(f), self.normal_head(f)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [*self.neck.parameters(), *self.albedo_head.parameters(), *self.shading_head.parameters(), \n",
    "                *self.normal_head.parameters(), *self.image_encoder.parameters()]\n",
    "\n",
    "    def train(self):\n",
    "        self.neck.train()\n",
    "        self.albedo_head.train()\n",
    "        self.shading_head.train()\n",
    "        self.normal_head.train()\n",
    "        self.image_encoder.train()\n",
    "        \n",
    "def get_optimizers(model, dataloader, lr, accelerator=None):\n",
    "  optimizer_class = torch.optim.AdamW\n",
    "  optimizer = optimizer_class(\n",
    "[      {\"params\": model.parameters(), \"lr\": lr}], eps=1e-5, weight_decay=0.03)\n",
    "\n",
    "  def lr_lambda(epoch):\n",
    "    return 1\n",
    "    # return math.pow(10, - epoch / decay_period)\n",
    "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "  if accelerator is not None:\n",
    "      model, dataloader, optimizer, scheduler = accelerator.prepare(model, dataloader, optimizer, scheduler)\n",
    "  return model, dataloader, optimizer, scheduler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "def debug_step(batch, results, running_loss, n, display_size):\n",
    "  gts = []\n",
    "  preds = []\n",
    "  with torch.no_grad():\n",
    "    for gt, pred in zip(batch, results):\n",
    "        gt = gt[0].permute((1, 2, 0)).detach().cpu().numpy() * 255\n",
    "        pred = pred[0].clamp(0, 1).permute((1, 2, 0)).detach().cpu().numpy() * 255\n",
    "        gt = Image.fromarray(gt.astype(np.uint8)).resize((display_size, display_size))\n",
    "        pred = Image.fromarray(pred.astype(np.uint8)).resize((display_size, display_size))\n",
    "        gts.append(gt)\n",
    "        preds.append(pred)\n",
    "  gts = np.hstack(gts)\n",
    "  preds = np.hstack(preds)\n",
    "  res = np.vstack([gts, preds]).astype(np.uint8)\n",
    "  print(running_loss)\n",
    "    \n",
    "  display(Image.fromarray(res))\n",
    "\n",
    "def compute_loss(x, y):\n",
    "    return (x - y).square().mean() + kornia.losses.ssim_loss(x, y, 11)\n",
    "    \n",
    "def train_parallel(train_config):\n",
    "  set_seed(42)\n",
    "  kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "  accelerator = Accelerator(mixed_precision=\"fp16\", kwargs_handlers=[kwargs], dynamo_backend=\"no\")\n",
    "\n",
    "  n_epochs = train_config.get(\"n_epochs\", 1)\n",
    "  lr = train_config.get(\"lr\", 1e-4)\n",
    "  decay_period = train_config.get(\"decay_period\", 5000)\n",
    "  n_epochs = train_config.get(\"n_epochs\", 1)\n",
    "  max_iterations = train_config.get(\"max_iterations\", None)\n",
    "  debug_interval = train_config.get(\"debug_interval\", 50)\n",
    "  save_interval = train_config.get(\"save_interval\", 1000)\n",
    "  display_size = train_config.get(\"display_size\", 128)\n",
    "  batch_size = train_config.get(\"batch_size\", 8)\n",
    "\n",
    "  ed = ElasticDino.from_pretrained(\"path/to/edino\", \"elasticdino-32-L\")\n",
    "  model = Model(ed)\n",
    "  dataloader = get_dataloader(hypersim_train_ds, batch_size)\n",
    "    \n",
    "  model, dataloader, optimizer, scheduler = get_optimizers(model, dataloader, lr, accelerator)\n",
    "    \n",
    "  running_loss = None\n",
    "  n = 0\n",
    "\n",
    "  print(\"Start training\")\n",
    "  for epoch in range(n_epochs):\n",
    "    print(\"Epoch\", epoch)\n",
    "    for img, albedo, shading, normal in dataloader:\n",
    "        if n == max_iterations:\n",
    "          return\n",
    "        img = img.to(device=accelerator.device)\n",
    "        albedo = albedo.to(device=accelerator.device)\n",
    "        shading = shading.to(device=accelerator.device)\n",
    "        normal = normal.to(device=accelerator.device)\n",
    "        n += 1\n",
    "        with accelerator.autocast():\n",
    "            pred_albedo, pred_shading, pred_normal = model(img)\n",
    "            loss = compute_loss(albedo, pred_albedo) + compute_loss(normal, pred_normal) + compute_loss(shading, pred_shading)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if running_loss is None:\n",
    "          running_loss = loss.item()\n",
    "        else:\n",
    "          running_loss = 0.98 * running_loss + 0.02 *  loss.item()  \n",
    "            \n",
    "        if n % debug_interval == 0 and accelerator.is_local_main_process:\n",
    "            debug_step([img, albedo, shading, normal], [img, pred_albedo, pred_shading, pred_normal], running_loss, n, display_size)\n",
    "\n",
    "        del img\n",
    "        del albedo\n",
    "        del shading\n",
    "        del normal\n",
    "        del loss\n",
    "        del pred_albedo\n",
    "        del pred_normal\n",
    "        del pred_shading\n",
    "        \n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "train_config = dict(\n",
    "  n_epochs=8,\n",
    "  # max_iterations=2,\n",
    "  lr = 1e-4,\n",
    "  decay_period=5000,\n",
    "  debug_interval=300,\n",
    "  save_interval=5,\n",
    "  display_size=128,\n",
    "  batch_size=16,\n",
    ")\n",
    "\n",
    "args = [train_config]\n",
    "\n",
    "notebook_launcher(\n",
    "  train_parallel,\n",
    "  args,\n",
    "  num_processes=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
